{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PathNet: Evolution Channels Gradient Descent in Super Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 싸이그래머/IAM : 파트 1 - 딥마인드 논문리뷰 [1]\n",
    "* 김무성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [2] DeepMind Blog - https://deepmind.com/research/publications/pathnet-evolution-channels-gradient-descent-super-neural-networks/\n",
    "* [3] DeepMind’s PathNet: A Modular Deep Learning Architecture for AGI - https://medium.com/intuitionmachine/pathnet-a-modular-deep-learning-architecture-for-agi-5302fcf53273#.1ovnxmikh\n",
    "* [4] DeepMind just published a mind blowing paper: PathNet - https://medium.com/@thoszymkowiak/deepmind-just-published-a-mind-blowing-paper-pathnet-f72b1ed38d46#.5o830hqgp\n",
    "* [5] (Vidoes) pathNet - Atari Transfer with A3C - https://www.youtube.com/watch?v=o9r-Z-sibS0&feature=youtu.be&list=PLKhLdFXp1JN5sHZOvJEuOjWF2Rsb2ONBv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "* ABSTRACT\n",
    "* 1 INTRODUCTION\n",
    "* 2 METHODS\n",
    "    - 2.1 PathNet Architecture\n",
    "    - 2.2 Pathway Evolution: Serial and Parallel\n",
    "    - 2.3 Transfer Learning Paradigm    \n",
    "    - 2.4 Binary MNIST classification tasks\n",
    "    - 2.5 CIFAR and SVHN classification tasks\n",
    "    - 2.6 Atari games\n",
    "    - 2.7 Labyrinth Games\n",
    "* 3 RESULTS\n",
    "    - 3.1 Binary MNIST Classification\n",
    "    - 3.2 CIFAR and SVHN\n",
    "    - 3.3 Atari Games\n",
    "    - 3.4 Labyrinth Games\n",
    "* 4 CONCLUSION\n",
    "* 5 ACKNOWLEDGMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABSTRACT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For artificial general intelligence (AGI) \n",
    "    - it would be efficient if \n",
    "        - <font color=\"red\">multiple users trained the same giant neural network</font>, \n",
    "            - permitting parameter reuse, \n",
    "            - without catastrophic forgetting.\n",
    "* <font color=\"red\">PathNet</font> is a first step in this direction.\n",
    "    - It is a <font color=\"red\">neural network algorithm</font> that\n",
    "        - <font color=\"red\">uses agents</font> \n",
    "            - <font color=\"red\">embedded in</font> the neural network \n",
    "        - whose task is \n",
    "            - to <font color=\"red\">discover</font> \n",
    "                - <font color=\"red\">which parts</font> of the network to \n",
    "                    - <font color=\"red\">re-use for new tasks</font>.\n",
    "* <font color=\"blue\">Agents are pathways</font> (views) \n",
    "    - through the network which \n",
    "        - <font color=\"blue\">determine</font> \n",
    "            - the <font color=\"blue\">subset of parameters</font> that \n",
    "                - are <font color=\"blue\">used and updated</font> \n",
    "                    - by the forwards and backwards passes of the backpropogation algorithm.\n",
    "    \n",
    "    <img src=\"figures/cap1.png\" width=500 />   \n",
    "* During learning, \n",
    "    - a <font color=\"red\">tournament selection genetic algorithm</font> is used \n",
    "        - to select pathways \n",
    "            - through the neural network for replication and mutation.  \n",
    "* <font color=\"red\">Pathway fitness</font> is \n",
    "    - the <font color=\"red\">performance of that pathway</font> \n",
    "        - measured according to a cost function.\n",
    "* We demonstrate <font color=\"blue\">successful transfer learning</font>; \n",
    "    - <font color=\"green\">fixing the parameters</font> \n",
    "        - <font color=\"green\">along a path learned on task A</font> and \n",
    "    - <font color=\"red\">re-evolving</font> \n",
    "        - a <font color=\"red\">new population of paths for task B</font>, \n",
    "            - allows task B \n",
    "                - to be <font color=\"red\">learned faster</font> than \n",
    "                    - it could be learned from scratch or after fine-tuning.    \n",
    "    - Paths evolved on task B re-use parts of the optimal path evolved on task A.\n",
    "* Positive transfer \n",
    "    - was demonstrated for \n",
    "        - binary MNIST, \n",
    "        - CIFAR, and \n",
    "        - SVHN supervised learning classification tasks, and \n",
    "    - a set of Atari and Labyrinth reinforcement learning tasks, \n",
    "    - suggesting PathNets have general applicability for neural network training. \n",
    "* Finally, PathNet also significantly improves the \n",
    "    - robustness to hyperparameter choices of \n",
    "        - a parallel asynchronous reinforcement learning algorithm (A3C)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keywords\n",
    "* Giant networks, \n",
    "* Path evolution algorithm\n",
    "<img src=\"figures/cap2.png\" width=400 />\n",
    "* Evolution and Learning \n",
    "<img src=\"http://www.ewh.ieee.org/soc/es/May2001/14/GAPROC0.GIF\" />\n",
    "* Continual Learning\n",
    "    - [6] Continual Learning and Deep Networks Workshop NIPS 2016 - https://sites.google.com/site/cldlnips2016/submissions\n",
    "* Transfer Learning\n",
    "    <img src=\"http://www.kdnuggets.com/wp-content/uploads/1_transfer-learning-indico.jpg\" width=400 />\n",
    "    <img src=\"https://image.slidesharecdn.com/dlcvd2l5transfer-160802094347/95/deep-learning-for-computer-vision-transfer-learning-and-domain-adaptation-upc-2016-4-638.jpg?cb=1470247790\" width=400 />\n",
    "    <img src=\"http://images.slideplayer.com/34/8370683/slides/slide_6.jpg\" width=600 />\n",
    "* MultiTask Learning\n",
    "    <img src=\"http://rinuboney.github.io/img/multi_task.png\" width=400 />\n",
    "    <img src=\"http://images.slideplayer.com/16/5263915/slides/slide_47.jpg\" width=400 />\n",
    "    <img src=\"http://www.cs.cornell.edu/~kilian/research/multitasklearning/files/pasted-graphic.jpg\" width=400 />\n",
    "    <img src=\"https://i.ytimg.com/vi/VChem2MrZgo/hqdefault.jpg\" width=400 />\n",
    "    <img src=\"https://www.iti.gr/~bmezaris/research/research_multitask.gif\" width=400 />\n",
    "    <img src=\"http://www.csmining.org/tl_files/research_pics/MTL_Richard.jpg\" width=400 />\n",
    "    <img src=\"http://gw.tnode.com/deep-learning/img/layer-wise-multi-task-learning.png\" width=400 />\n",
    "* Basal Ganglia\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/33/BrainCaudatePutamen.svg/1200px-BrainCaudatePutamen.svg.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A plausible requirement for artificial general intelligence is that many users will be required to train the same giant neural network on a multitude of tasks. \n",
    "* This is the most efficient way for the network to gain experience, because such a network can reuse existing knowledge instead of learning from scratch for each task. \n",
    "* <font color=\"red\">To achieve this, we propose that each user of the giant net be given a population of agents whose job it is to learn the user-defined task as efficiently as possible</font>. \n",
    "* Agents will learn how best to re-use existing parameters in the environment of the neural network by executing actions within the neural network. \n",
    "* They must work in parallel with other agents who are learning other tasks for other users, sharing parameters if transfer is possible, learning to update disjoint parameters if interference is significant. \n",
    "* Each agent may itself be controlled by an arbitrarily complex reinforcement learning algorithm, but here <font color=\"red\">we chose the very simplest possible ‘agent’, a unit of evolution</font>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap1.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [3] DeepMind’s PathNet: A Modular Deep Learning Architecture for AGI - https://medium.com/intuitionmachine/pathnet-a-modular-deep-learning-architecture-for-agi-5302fcf53273#.1ovnxmikh\n",
    "* [7] Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer - https://arxiv.org/abs/1701.06538\n",
    "* [8] Convolutional Neural Fabrics - https://arxiv.org/abs/1606.02492"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a standard neural network is naively trained training cost scales quadratically with model width, whereas <font color=\"red\">PathNet theoretically has constant computation speed with respect to the network width because only a fixed-size subset of the larger network is used for the forwards and backwards pass at any time</font> (although there is no guarantee that more training may not be required in some cases)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outrageously large neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our work shares a motivation with a recent paper “Outrageously large neural networks” in which the authors write that “the capacity of a neural network to absorb information is limited by its number of parameters”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*GbSb-hrRhIPDiyHysa69kQ.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Neural Fabrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our work is related also to “Convolutional Neural Fabrics” in which connection strengths between modules in the fabric are learned, but where (unlike PathNet) the whole fabric is used all the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*ezkmJyQgBwCBMgGQXP_A6g.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tournament selection genetic algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A tournament selection genetic algorithm is then used to evolve paths, where during a <font color=\"blue\">fitness evaluation the path is trained for a few game episodes by gradient descent using a reinforcement learning algorithm</font>. \n",
    "* Thus, <font color=\"red\">evolution and learning are taking place simultaneously, with evolution only guiding where gradient descent should be applied to change the weight and bias parameters</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap1.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 1 illustrates the algorithm in action. \n",
    "* The first task A is Pong and the second task B is Alien; \n",
    "    - both are trained consecutively for 80M timesteps each.\n",
    "* The purple lines in Box 1 \n",
    "    - of the figure shows all 64 randomly initialized paths through the neural network model at the beginning of Pong training\n",
    "* Box 2 \n",
    "    - shows the population converging (with many paths overlapping with each other) as performance improves.\n",
    "* Box 3\n",
    "    - As perfect perfor- mance is achieved the population converges to a single path as shown in Box 3\n",
    "* Box 4\n",
    "    - Box 4 shows that the converged single pathway persists until the end of the training session.\n",
    "    - At this point the task switches to task B (Alien) and the op- timal path for Pong gets ’fixed’, i.e. the modules on that path have their weights and biases frozen.\n",
    "* Box 5\n",
    "    - Box 5 shows the fixed path as heavy dark red lines alongside a new randomly initialized population of paths in light blue.\n",
    "* Box 8 \n",
    "    - The new population of paths evolve and converge on Alien by Box 8.\n",
    "* Box 9\n",
    "    - After 160M steps, the optimal path for Alien was fixed, shown as dark blue lines in Box 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### catastrophic forgetting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* PathNets evolve a population of pathways through a neu- ral network that scaffolds and channels any desired gradient-descent-based learning algorithm towards a limited subset of the neural network’s parameters and then fixes these parameters after learning so that functionality can never be lost\n",
    "* PathNets allow the relationships between the original ‘columns’ and later ‘columns’ to be evolved, where a column is one deep neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Darwinian Neurodynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The concept of the PathNet was first conceived of within the framework of Darwinian Neurodynamics as an attempt to envisage how evolutionary algorithms could be implemented in the brain. \n",
    "* However, in that original work both the topology and the weights of the path were evolved and there was no gradient descent learning. \n",
    "* Performance was comparable with a standard genetic algorithm on combinatorial optimization problems. \n",
    "* Here we show performance superior to A3C and stochastic gradient descent for transfer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. METHODS\n",
    "* 2.1 PathNet Architecture\n",
    "* 2.2 Pathway Evolution: Serial and Parallel\n",
    "* 2.3 Transfer Learning Paradigm \n",
    "* 2.4 Binary MNIST classification tasks\n",
    "* 2.5 CIFAR and SVHN classification tasks\n",
    "* 2.6 Atari games\n",
    "* 2.7 Labyrinth Games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 PathNet Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A PathNet is a modular deep neural network having L layers with each layer consisting of M modules. \n",
    "* Each module is itself a neural network, \n",
    "    - here either convolutional or linear, followed by a transfer function; \n",
    "        - rectified linear units are used here. \n",
    "* For each layer the outputs of the modules in that layer are summed before being passed into the active modules of the next layer. \n",
    "* A module is active if it is present in the path genotype currently being evaluated \n",
    "    - A maximum of N distinct modules per layer are permitted in a pathway (typically N = 3 or 4). \n",
    "* The final layer is unique and unshared for each task being learned. \n",
    "* In the supervised case this is a single linear layer for each task, and in the A3C case each task (e.g. Atari game) has a value function readout and a policy readout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap2.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Pathway Evolution: Serial and Parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Serial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* P genotypes (pathways) are initialized randomly, \n",
    "    - each genotype is at most a N by L matrix of integers, \n",
    "        - which describe the active modules in each layer in that pathway. \n",
    "* In the serial supervised implementation, \n",
    "    - a binary tournament selection algorithm is implemented in series as follows. \n",
    "        - A random genotype is chosen, \n",
    "        - and its pathway is trained for T epochs, \n",
    "        - its fitness being the negative classification error during that period of training.\n",
    "        - Then another random genotype is chosen and its pathway trained for T epochs. \n",
    "        - A copy of the winning pathway’s genotype overwrites the losing pathways genotype.\n",
    "        - The copy of the winning pathway’s genotype is then mutated by choosing independently each element with a probability of 1/[N × L] and adding an integer in the range [−2, 2] to it.\n",
    "        - A local neighbourhood was used to promote spatial localization of network functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the A3C (reinforcement learning) case, \n",
    "    - all 64 genotypes are evaluated in parallel, \n",
    "        - one by each of the 64 workers. \n",
    "    - Therefore pathways restrict the simultaneous updates of parameters by the workers to only their specific subsets, \n",
    "        - as opposed to the standard A3C algorithm in which all workers update all parameters.\n",
    "    - The fitness of a genotype is the return accumulated over the T episodes that\n",
    "        - a worker played using that genotypes pathway. \n",
    "    - While a worker is evaluating, \n",
    "        - it writes a large negative fitness to the shared fitness array, \n",
    "        - so that no genotypes wins a tournament until it has been evaluated. \n",
    "    - Once the worker has finished T episodes, \n",
    "        - it chooses B other random genotypes and \n",
    "        - checks if any of those genotypes have returned a fitness of at least its own fitness. \n",
    "    - If at least one has, \n",
    "        - then the highest fit genotype overwrites the current worker’s genotype, and is mutated as above. \n",
    "    - If no other worker had a genotype with fitness greater than this workers own genotype,\n",
    "        - then the worker reevaluates its own genotype."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Transfer Learning Paradigm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=\"red\">Once task A has been trained</font> for a fixed period of time or until some error threshold has been reached, the <font color=\"red\">best fit pathway is fixed, which means its parameters are no longer allowed to change</font>. \n",
    "* All other parameters <font color=\"blue\">not in an optimal path are reinitialized</font>. \n",
    "* <font color=\"red\">We found that without reinitialization transfer performance did not exceed that of fine-tuning</font>. \n",
    "* In the A3C case (but not the supervised learning cases) the original best fit pathway is always active during the forwards pass of the network, in addition to the newly evolving pathway, but its parameters are not modified by the backwards pass.\n",
    "* A new set of random pathways is then initialized and evolved/trained on task B.\n",
    "* In both the supervised and reinforcement settings, <font color=\"red\">pathNet is compared with two alternative setups</font>: \n",
    "    - an <font color=\"red\">independent learning control</font> where the target task is learned de novo, and \n",
    "    - a <font color=\"red\">fine-tuning control where the second task is learned with the same path that learned the first task</font> (but with a new value function and policy readout)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Binary MNIST classification tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A binary MNIST classification involves distinguishing two classes of MNIST digits from one another, for example 5 verses 6 [11]. To make the task more difficult, salt and pepper noise of 50% is added to the MNIST digits. A transfer experiment involves training and evolving paths on the first task until perfect classification on the training set is achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 CIFAR and SVHN classification tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The larger version of the above network is used to train on CIFAR and cropped SVHN of standard size 28 × 28 withL = 3 and M = 20 modules per layer of 20 neurons each, and with pathways that may contain up to 5 modules per layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Atari games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We tested whether there was a speedup in learning a second (target) game after having played either Pong, River-Raid or Seaquest as a source game. \n",
    "* The target games were as follows: Alien, Asterix, Boxing, Centipede, Gopher, Hero, JamesBond, Krull, RoadRunner, StarGunner, WizardofWor. \n",
    "* These are the same games presented by the authors of Progressive Neural Networks. In this case the A3C algorithm was used with 64 workers running in parallel. \n",
    "* Figure 2 shows the network architecture: \n",
    "    - The first three layers’ modules are shown as green boxes; \n",
    "    - final layer of modules shown as purple boxes; \n",
    "    - the between-layer summing modules are shown as blue boxes; and \n",
    "    - active modules specified by the pathway as red boxes. \n",
    "    - Readout units not involved in the PathNet itself are shown as circles on the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap2.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Labyrinth Games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The PathNet architecture used for Labyrinth was identical to the one used for Atari with the exception of a module- duplication mechanism. \n",
    "* In the previous models there has not been any actual copying of parameters, only copying of views over a fixed set of parameters. \n",
    "* <font color=\"red\">In the Labyrinth model we enabled PathNet to copy the weights of modules to other modules within the same layer, emulating Net2Net or a distillation like operation</font>. \n",
    "* To aid this, we measure the extent to which a module contributes to the fitness of all the paths it is in by taking a sliding mean over the fitness of any path which contains that module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. RESULTS\n",
    "* 3.1 Binary MNIST Classification\n",
    "* 3.2 CIFAR and SVHN\n",
    "* 3.3 Atari Games\n",
    "* 3.4 Labyrinth Games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Binary MNIST Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap3.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis did not reveal that the speedup ratio was correlated with path overlap as measured by the number of modules in the original optimal path that were present in the population of paths at the end of the second task. <font color=\"red\">This suggests that speedup can be obtained by PathNet both determining when there should be overlap and when there should not be overlap</font>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap4.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 CIFAR and SVHN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap5.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Atari Games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Figure 6 the performance of PathNet (blue) is compared with independent and fine-tuning controls (green) on 4 target games, having learned RiverRaid first. In all cases the top 5 runs of a hyperparameter search are shown. We generally found that strong selection with tournament sizes of B = 10, T = 10 game episodes per evaluation, and low mutation rates of 0.01-0.001 were optimal, allowing rapid convergence of paths to a single path, followed by explo- ration of small variants to a path, thus focusing learning on a few parameters, with occasional exploration of novel by- passes. PathNet is superior to controls in these 4 cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Figure 7 shows that PathNet is on average superior to independent learning and fine-tuning over the Atari games we investigated</font>. Compared to a control of 1.0, fine tun- ing achieves a 1.16 speedup on average whereas PathNet achieves a 1.33 times speedup. Results for transfer on more Atari games can be seen in Figure 12."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap7.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12-1.png\" width=800 />\n",
    "<img src=\"figures/cap12-2.png\" width=800 />\n",
    "<img src=\"figures/cap12-3.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Labyrinth Games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PathNet transfer between the three labyrinth games lt chasm, seekavoid arena and stairway to melon is compared with fixed maximum-size-path de novo training and fine-tuning con- trols for which a hyperparameter sweep was conducted involving mutation rates [ 0.1, 0.01, 0.001], module duplication rate [ 0.0, 0.05, 0.1, 0.5] (per episode completed by worker 0) and tournament size B [2, 10]. The learning rate was fixed at 0.001, entropy cost at 0.0001 and evaluation time T at 13.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 9 shows the mean of the training performance for the top 5 experiments for both PathNet and the fixed-path controls. Each row represents a different source game and each column a different target game. Where the source game and target game are the same the graph shows the results of learning the game de novo from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 11 shows that in some cases the module duplication operation produces improved performance compared to standard PathNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap11.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of the best runs from a hyperparameter search are summarized in Figure 10. Here performance is evaluated by measuring the area under the learning curve (average score per episode during training), rather than final score. The numbers in the table show the relative performance of an architecture learning a target task (each column) compared with an independent baseline with a fixed maximum size path trained from scratch only on the target task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also compared PathNet to independent and fine-tuning controls over the same sweep of 243 hyperparameters as used in the Atari experiments described above. On the Labyrinth level seekavoid arena in which the agent must collect apples but avoid lemons we found that the PathNet had significantly higher mean performance than control runs, both when learning seekavoid arena from scratch compared to the de novo controls, and for relearning from the same task, compared to fine-tuning from a network that had previously learned seekavoid arena, see Figure 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. CONCLUSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Path Evolution Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PathNet extends our original work on the Path Evolution Algorithm to Deep Learning whereby the weights and biases of the network are learned by gradient descent, but evolution determines which subset of parameters is to be trained. We have shown that PathNet is capable of sustaining transfer learning on at least four tasks in both the supervised and reinforcement learning settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evolutionary dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PathNet may be thought of as implementing a form of ‘evolutionary dropout’ in which instead of randomly dropping out units and their connections, dropout samples or ‘thinned networks’ are evolved. PathNet has the added advantage that dropout frequency is emergent, because the population converges faster at the early layers of the network than in the later layers. PathNet also resembles ‘evolutionary swapout’, in fact we have experimented with having standard linear modules, skip modules and residual modules in the same layer and found that path evolution was capable of discovering effective structures within this diverse network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### convolutional neural fabrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PathNet is related also to recent work on convolutional neural fabrics, but there the whole network is always used and so the principle cannot scale to giant networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parameter copying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other approaches to combining evolution and learning have involved parameter copying, whereas there is no such copying in the current implementation of PathNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### futures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Whilst we have only demonstrated PathNet in a fairly small network, the principle can scale to much larger neural networks with more efficient implementations of pathway gating. This will allow extension to multiple tasks. \n",
    "* We also wish to try PathNet on other RL tasks which may be more suitable for transfer learning than Atari, for example con- tinuous robotic control problems. \n",
    "* Further investigation is required to understand the extent to which PathNet may be superior to using fixed paths.\n",
    "* We are still investigating the potential benefits of module duplication. \n",
    "* Finally, it is always possible and sometimes desirable to replace evolutionary variation operators with variation operators learned by reinforcement learning. \n",
    "     - A tournament selection algorithm with mutation is only the simplest way to achieve adaptive paths. \n",
    "     - It is clear that more sophisticated methods such as policy gradient methods may be used to learn the distribution of pathways as a function of the long term returns obtained by paths, and as a function of a task description input. \n",
    "     - This may be done through a softer form of gating than used by PathNet here.\n",
    "     - Furthermore, a population (ensemble) of soft gate matrices may be maintained and an RL algorithm may be permitted to ’mutate’ these values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basal Ganglia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 참고\n",
    "* [9] Towards an executive without a homunculus: computational models of the prefrontal cortex/basal ganglia system -  http://www.igi.tugraz.at/lehre/SeminarA/SS09/haeusler_A_2009.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The operations of PathNet resemble those of the Basal Ganglia, which we propose determines which subsets of the cortex are to be active and trainable as a function of goal/sub-goal signals from the prefrontal cortex, a hypothesis related to others in the literature (original paper references [8] [14] [6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. ACKNOWLEDGMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고자료\n",
    "* [1] (Paper) PathNet: Evolution Channels Gradient Descent in Super Neural Networks - https://arxiv.org/abs/1701.08734\n",
    "* [2] DeepMind Blog - https://deepmind.com/research/publications/pathnet-evolution-channels-gradient-descent-super-neural-networks/\n",
    "* [3] DeepMind’s PathNet: A Modular Deep Learning Architecture for AGI - https://medium.com/intuitionmachine/pathnet-a-modular-deep-learning-architecture-for-agi-5302fcf53273#.1ovnxmikh\n",
    "* [4] DeepMind just published a mind blowing paper: PathNet - https://medium.com/@thoszymkowiak/deepmind-just-published-a-mind-blowing-paper-pathnet-f72b1ed38d46#.5o830hqgp\n",
    "* [5] (Vidoes) pathNet - Atari Transfer with A3C - https://www.youtube.com/watch?v=o9r-Z-sibS0&feature=youtu.be&list=PLKhLdFXp1JN5sHZOvJEuOjWF2Rsb2ONBv\n",
    "* [6] Continual Learning and Deep Networks Workshop NIPS 2016 - https://sites.google.com/site/cldlnips2016/submissions\n",
    "* [7] Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer - https://arxiv.org/abs/1701.06538\n",
    "* [8] Convolutional Neural Fabrics - https://arxiv.org/abs/1606.02492\n",
    "* [9] Towards an executive without a homunculus: computational models of the prefrontal cortex/basal ganglia system -  http://www.igi.tugraz.at/lehre/SeminarA/SS09/haeusler_A_2009.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
