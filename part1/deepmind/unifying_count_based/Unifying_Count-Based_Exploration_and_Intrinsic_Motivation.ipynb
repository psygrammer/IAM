{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unifying Count-Based Exploration and Intrinsic Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 싸이그래머 / IAM - 파트 1 : 딥마인드 논문 리뷰 [1]\n",
    "* 김무성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "* Abstract\n",
    "* 1 Introduction\n",
    "* 2 Notation\n",
    "* 3 From Densities to Counts\n",
    "    - 3.1 Pseudo-Counts and the Recoding Probability\n",
    "    - 3.2 Estimating the Frequency of a Salient Event in FREEWAY\n",
    "* 4 The Connection to Intrinsic Motivation\n",
    "* 5 Asymptotic Analysis\n",
    "* 6 Empirical Evaluation\n",
    "    - 6.1 Exploration in Hard Atari 2600 Games\n",
    "    - 6.2 Exploration for Actor-Critic Methods\n",
    "* 7 Related Work\n",
    "* 8 Future Directions\n",
    "* C Experimental Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [2] http://www.slideshare.net/ItsukaraIitsuka/drl-challenge-on-montezumas-revenge\n",
    "* [3] http://www.slideshare.net/KatsukiOhto/unifying-count-based-exploration-and-intrinsic-motivation\n",
    "* [4] https://www.youtube.com/watch?v=0yI2wJ6F8r0\n",
    "* [5] https://gym.openai.com/evaluations/eval_irmpzSknRMAjzv2yhseNA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We consider an agent’s uncertainty about its environment and the problem of generalizing this uncertainty across states. \n",
    "* Specifically, we focus on the problem of exploration in non-tabular reinforcement learning. \n",
    "* Drawing inspiration from the <font color=\"red\">intrinsic motivation</font> literature, we use <font color=\"red\">density models to measure uncertainty</font>, and propose a novel algorithm for <font color=\"red\">deriving a pseudo-count</font> from an arbitrary density model.\n",
    "* This technique enables us to <font color=\"red\">generalize count-based exploration algorithms to the non-tabular case</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploration algorithms for Markov Decision Processes (MDPs) are typically concerned with <font color=\"red\">reducing the agent’s uncertainty</font> <font color=\"blue\">over the environment’s reward and transition functions</font>.\n",
    "* In a tabular setting, \n",
    "    - this uncertainty can be quantified using \n",
    "        - confidence intervals derived from Chernoff bounds, or \n",
    "        - inferred from a posterior over the environment parameters.\n",
    "* In fact, both confidence intervals and posterior shrink \n",
    "    - as the inverse square root of \n",
    "        - the state-action visit count N (x, a), \n",
    "    - making this quantity fundamental to most theoretical results on exploration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### count-based exploration method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count-based exploration methods directly use visit counts to guide an agent’s behaviour towards reducing uncertainty. For example, Model-based Interval Estimation with <font color=\"red\">Exploration Bonuses</font> (MBIE-EB; Strehl and Littman, 2008) solves the augmented Bellman equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap1.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "involving the empirical reward Rˆ, the empirical transition function Pˆ, and an exploration bonus proportional to $N(x,a)^{−1/2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In spite of their pleasant theoretical guarantees, <font color=\"red\">count-based methods have not played a role</font> in the contemporary successes of reinforcement learning (e.g. Mnih et al., 2015). <font color=\"blue\">Instead, most practical methods still rely on simple rules such as ε-greedy</font>. \n",
    "\n",
    "<font color=\"red\">The issue is that visit counts are not directly useful in large domains, where states are rarely visited more than once</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### intrinsic motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answering a different scientific question, intrinsic motivation aims to provide qualitative guidance for exploration (Schmidhuber, 1991; Oudeyer et al., 2007; Barto, 2013). This guidance can be summarized as “<font color=\"red\">explore what surprises you</font>”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical approach guides the agent based on change in <font color=\"red\">prediction error, or learning progress</font>. \n",
    "\n",
    "If en(A) is the error made by the agent at time n over some event A, and en+1(A) the same error after observing a new piece of information, then learning progress is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap2.png\" width=200 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intrinsic motivation methods are <font color=\"red\">attractive as they remain applicable in the absence of the Markov property or the lack of a tabular representation</font>, both of which are required by count-based algorithms. Yet the theoretical foundations of intrinsic motivation remain largely absent from the literature, which may explain its slow rate of adoption as a standard approach to exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pseudo-count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this paper we provide formal evidence that <font color=\"red\">intrinsic motivation and count-based exploration are but two sides of the same coin</font>.\n",
    "* information gain\n",
    "    - Specifically, we consider a frequently used measure of learning progress, information gain (Cover and Thomas, 1991). Defined as the Kullback-Leibler divergence of a prior distribution from its posterior, information gain can be related to the confidence intervals used in count-based exploration.\n",
    "* pseudo-count\n",
    "    - Our contribution is to propose a new quantity, the pseudo-count, which connects information-gain-as-learning-progress and count-based exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">We derive our pseudo-count from a density model over the state space</font>. This is in departure from more traditional approaches to intrinsic motivation that consider learning progress with respect to a transition model. We expose the relationship between pseudo-counts, a variant of Schmidhuber’s compression progress we call prediction gain, and information gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">The pseudo-counts we introduce here are best thought of as “function approximation for exploration”</font>. We bring them to bear on Atari 2600 games from the Arcade Learning Environment (Bellemare et al., 2013), focusing on games where myopic exploration fails. <font color=\"red\">We extract our pseudo-counts from a simple density model and use them</font> within a variant of MBIE-EB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap3.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 From Densities to Counts\n",
    "* 3.1 Pseudo-Counts and the Recoding Probability\n",
    "* 3.2 Estimating the Frequency of a Salient Event in FREEWAY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Pseudo-Counts and the Recoding Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap4.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Estimating the Frequency of a Salient Event in FREEWAY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap5.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an illustrative example, we employ our method to estimate the number of occurrences of an infrequent event in the Atari 2600 video game FREEWAY (Figure 1, screenshot). We use the Arcade Learning Environment (Bellemare et al., 2013). We will demonstrate the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Pseudo-counts are roughly zero for novel events,\n",
    "2. they exhibit credible magnitudes,\n",
    "3. they respect the ordering of state frequency,\n",
    "4. they grow linearly (on average) with real counts,\n",
    "5. they are robust in the presence of nonstationary data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CTS model\n",
    "<font color=\"red\">We use a simplified, pixel-level version of the CTS model</font> for Atari 2600 frames proposed by Belle- mare et al. (2014), ignoring temporal dependencies. While the CTS model is rather impoverished in comparison to state-of-the-art density models for images (e.g. Van den Oord et al., 2016), its <font color=\"red\">count-based nature results in extremely fast learning, making it an appealing candidate for exploration</font>. Further details on the model may be found in the appendix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 The Connection to Intrinsic Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having argued that <font color=\"red\">pseudo-counts appropriately generalize visit counts</font>, we will now show that they are closely related to <font color=\"red\">information gain</font>, which is commonly used to <font color=\"red\">quantify novelty or curiosity</font> and consequently <font color=\"red\">as an intrinsic reward</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information gain is defined in relation to a mixture model ξ over a class of density models M. This model predicts according to a weighted combination from M:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap7.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theorem 1 suggests that using an exploration bonus proportional to Nˆn(x)−1/2, similar to the MBIE-EB bonus, leads to a behaviour at least as exploratory as one derived from an information gain bonus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, it may be difficult to provide theoretical guarantees about existing bonus-based intrinsic motivation approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike many intrinsic motivation algorithms, pseudo-counts also do not rely on learning a forward (transition and/or reward) model. This point is especially important because a number of powerful density models for images exist (Van den Oord et al., 2016), and because optimality guarantees cannot in general exist for intrinsic motivation algorithms based on forward models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Asymptotic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we analyze the limiting behaviour of the ratio Nˆn/Nn. We use this analysis to assert the consistency of pseudo-counts derived from tabular density models, i.e. models which maintain per-state visit counts. In the appendix we use the same result to bound the approximation error of pseudo-counts derived from directed graphical models, of which our CTS model is a special case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap11.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Empirical Evaluation\n",
    "* 6.1 Exploration in Hard Atari 2600 Games\n",
    "* 6.2 Exploration for Actor-Critic Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Exploration in Hard Atari 2600 Games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap13.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap14.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Exploration for Actor-Critic Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next used our exploration bonuses in conjunction with the A3C (Asynchronous Advantage Actor-Critic) algorithm of Mnih et al. (2016). One appeal of actor-critic methods is their explicit separation of policy and Q-function parameters, which leads to a richer behaviour space. This very separation, however, often leads to deficient exploration: to produce any sensible results, the A3C policy must be regularized with an entropy cost. We trained A3C on 60 Atari 2600 games, with and without the exploration bonus (4). We refer to our augmented algorithm as A3C+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap15.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Related Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intrinsic motivation has also been studied in reinforcement learning proper, in particular in the con- text of discovering skills (Singh et al., 2004; Barto, 2013). Recently, Stadie et al. (2015) used a squared prediction error bonus for exploring in Atari 2600 games. Closest to our work is Houthooft et al. (2016)’s variational approach to intrinsic motivation, which is equivalent to a second order Taylor approximation to prediction gain. Mohamed and Rezende (2015) also considered a variational approach to the different problem of maximizing an agent’s ability to influence its environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Future Directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last few years have seen tremendous advances in learning representations for reinforcement learning. Surprisingly, these advances have yet to carry over to the problem of exploration. In this paper, we reconciled counts, the fundamental unit of uncertainty, with prediction-based heuristics and intrinsic motivation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Induced metric. \n",
    "* We did not address the question of where the generalization comes from. Clearly, the choice of density model induces a particular metric over the state space. A better understanding of this metric should allow us to tailor the density model to the problem of exploration.\n",
    "\n",
    "#### Compatible value function. \n",
    "* There may be a mismatch in the learning rates of the density model and the value function: DQN learns much more slowly than our CTS model. As such, it should be beneficial to design value functions compatible with density models (or vice-versa).\n",
    "\n",
    "#### The continuous case.\n",
    "* Although we focused here on countable state spaces, we can as easily define a pseudo-count in terms of probability density functions. At present it is unclear whether this provides us with the right notion of counts for continuous spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C Experimental Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap16.png\" width=800 />\n",
    "<img src=\"figures/cap17.png\" width=800 />\n",
    "<img src=\"figures/cap18.png\" width=800 />\n",
    "<img src=\"figures/cap19.png\" width=800 />\n",
    "<img src=\"figures/cap20.png\" width=800 />\n",
    "<img src=\"figures/cap21.png\" width=800 />\n",
    "<img src=\"figures/cap22.png\" width=800 />\n",
    "<img src=\"figures/cap23.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고자료\n",
    "* [1] (paper) Unifying Count-Based Exploration and Intrinsic Motivation - https://arxiv.org/abs/1606.01868\n",
    "* [2] http://www.slideshare.net/ItsukaraIitsuka/drl-challenge-on-montezumas-revenge\n",
    "* [3] http://www.slideshare.net/KatsukiOhto/unifying-count-based-exploration-and-intrinsic-motivation\n",
    "* [4] https://www.youtube.com/watch?v=0yI2wJ6F8r0\n",
    "* [5] https://gym.openai.com/evaluations/eval_irmpzSknRMAjzv2yhseNA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
